{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36c34af-36e8-483f-90dd-bc10634ebbd2",
   "metadata": {
    "id": "e36c34af-36e8-483f-90dd-bc10634ebbd2",
    "tags": []
   },
   "source": [
    "## AAI30001 Small Project\n",
    "#### **Group: SP_8**\n",
    " - Chua Chen Yi (2302822)\n",
    " - Wong Jun Jai (2302765)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee5ec1-a889-4910-a57a-014952aabfb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318d2a8-30c2-4751-81b4-04d809b39d35",
   "metadata": {
    "id": "1318d2a8-30c2-4751-81b4-04d809b39d35",
    "tags": []
   },
   "source": [
    "#### Our proposed method of improving accuracy over the baseline score is as follows:\n",
    "### 1. Identify a pre-trained model to be our baseline for fine-tuning\n",
    "- Before starting our search, we manually re-created the same testing environment as the sample code. This included splitting the dataset into training, validation and testing identically to the sample. In addition, the use of the TestUA for determining overall performance was use\n",
    "    - *Minor note: We have noticed the way the dataset is split for testing is not the same as how it is described in the text. For example, the test set should only contain files from 'Ses01F' and only from the female speaker. However, checking the dataset class showed that this is not true.*\n",
    "- With a simple evaluation pipeline in placee, we randomly choose 10 different models publically availble from hugging face, and proceeded to score them. They scored a range from 0.60-0.80 on the test dataset.\n",
    "- We decided to use the same base model: **\"facebook/wav2vec2-base\"** with the goal of fine tuning according to the rules and achieve at least 0.70\n",
    "### 2A. Perform Data Augmentation On Dataset And Fine Tune Using Grid Search\n",
    "- #### Augmentation 1\n",
    "    - Details\n",
    "- #### Augmentation 2\n",
    "    - Details\n",
    "    \n",
    "### 2B. Fine Tune Using Grid Search\n",
    "- #### Tested:\n",
    "    \n",
    "### 3. Identify Strengths & Weakness of Pre-Trained Model\n",
    "- We started by training the model for a few epochs to get a general idea of how it performs. A sample confusion matrix is shown here:\n",
    "\n",
    "             A   H    N   S\n",
    "        A  103   8   35   1\n",
    "        H    8  79   36   9\n",
    "        N    2   7  109  53 <- Highest Error\n",
    "        S    1   8   10  59\n",
    "        \n",
    "    In general, the model is able to differentiate angry and happy emotions with a high degree of accuracy. However, the model is not good at differentiaing between neutral and sad emotions. Its greatest weakness is predicting a neutral emotion as a sad one\n",
    "                \n",
    "### 4. Use Secondary Model With Text Embeddings\n",
    "- We have decided not to use extracted text embeddings as a feature of our first model, but instead have a completely seperate model extract and perform sentiment analysis on the text. The final prediction will be a combination of both models.\n",
    "    - This method allows us to:\n",
    "        1. Manage our time better as work can be done to improve the performance of both indepenently.\n",
    "        2. Change our base model if we find a better one.\n",
    "        3. Choose more sophisticated speech-to-text and sentiment analysis models\n",
    "### 5. Develop Algorithm To Merge Predictions\n",
    "- Our final implementation consist of getting the confidence for each label using softmax in addition to its original prediction. The predictions for the 2nd model is then merged into a single CSV file. A gridsearch-like function will identify the optimal parameters *(highlighted in **Bold**)*.\n",
    "- All models tested have improved scores. In general, models with <0.65 score will see a boost of 2-4%, while models >0.65 score will gain 0-2%. We have also tried training a binary classifier for just sad and not sad, however most of the time the accuracy actually decreased.\n",
    "    ### Merge Algorithm\n",
    "    - The algorithm that determines the final predictions is a combination of 3 different strategies:\n",
    "        1. **Merging Strategy:** When to rely on the 2nd model?\n",
    "        2. **Prediction Strategy:** How to rely on the 2nd model?\n",
    "        3. **Mapping Strategy:** How to map sentiment to emotions?\n",
    "    ### 5A. Merging Strategy\n",
    "    We have identified 2 possible metrics to decide when to rely on the 2nd model\n",
    "    - **Entrophy Threshold**\n",
    "    We apply a calculate the entrophy based on the 4 confidence scores, following the logic that a lower overall entrophy will mean the model is most confident in its prediction. We will refer to the 2nd model when the entrophy is above the ***entrophy_threshold***.\n",
    "    - **Argmax Threshold**\n",
    "    We apply a simple argmax on the 4 confidence scores. If the value is below the ***argmax_threshold***, we will refer to the 2nd model for the final prediction\n",
    "    ### 5B. Prediction Strategy\n",
    "    We have identified 3 possible metrics to decide how to rely on the 2nd model\n",
    "    - **Default**\n",
    "    Prefer prediction of 2nd model in all situations\n",
    "    - **Ignore**\n",
    "    We identified the original model is very good at angry and happy emotions. So we always prefer the orignal model's predictions if it detects angry or happy\n",
    "    - **Ignore When Match**\n",
    "    If both models agree on the same prediction, we ignore however low the confidence is and assume is correct\n",
    "    ### 5C. Mapping Strategy\n",
    "    Because our sentiment analysis outputs 3 classes, while we have 4 emotions, we ill need to map 1 class to 2 emotions. This corresponds to the 'Negative' sentiment being mapped to either 'Angry' or 'Sad'. We have implemented the following methods:\n",
    "    - **Simple Mapping**\n",
    "    We decide on a ***sentiment_threshold*** value, where anything above is 'Sad'. This can also ***fliped*** around, ie. anything above is 'Angry' as there is no defined way to map the negative sentiment\n",
    "    - **Reference Mapping**\n",
    "    When a negative sentiment needs to be mapped, the confidence of 'Sad' and 'Angry' from the original model is looked up, and Argmax is used to return the most likely emotion.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73505ae0-7b64-4e97-94d6-1abb0d66e29d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## User Configurable Notebook Settings & Parameters\n",
    "***Everything that can be set and changed will be in here***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c1166-fece-43fe-86e7-ecf69b6ab1ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cloud or Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1690c1-9a34-431f-82b2-cab0da0842c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD = False #Set true for local Jupyter instance. Provide filepath if false\n",
    "CLOUD_TSV_LOCATION = '/kaggle/input/small-project/small-project/IEMOCAP_4.tsv'\n",
    "CLOUD_AUDIO_DIRECTORY = '/kaggle/input/small-project/small-project/IEMOCAP_full_release_audio'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7590ce5-6a9e-438e-a623-46540913b2c7",
   "metadata": {},
   "source": [
    "#### Notebook Run Mode\n",
    "*Please set only one option to true!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563c984-c751-4e27-a8d0-4f46038eb288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_MODE = True # Runs the entire notebook with default settings. Loads a pretrained model, finetunes, predicts and output\n",
    "BEST_MODEL_MODE = False #Loads our best trained model. Continues notebook as normal afterwards\n",
    "BEST_MODEL_PATH = r'C:\\Users\\ChenYi\\Documents\\Github\\AAI3001-Small-Project\\Archive\\Best 0.7433\\wav2vec2-updated-dataloader'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d35bd5-c79a-4415-b988-288a0efbf97f",
   "metadata": {},
   "source": [
    "#### Output Options (Graphs/Plots/Prints/Folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f252023-0103-4204-b42b-90ab936f32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_TRAINING_GRAPHS = True\n",
    "OUTPUT_MODEL_NAME = \"wav2vec2-AAI3001-NLP-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b158e5e2-2ba8-48c7-81f1-65ae5e92bf7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Merge Algorithm Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829aa96-9de1-42fb-a764-565ac1d77e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of 0 to 2\n",
    "enthropy_start = 0.6\n",
    "enthropy_stop = 1.2\n",
    "enthropy_step = 0.01\n",
    "\n",
    "# Values of 0 to 1\n",
    "sentiment_start = 0.3\n",
    "sentiment_stop = 0.8\n",
    "sentiment_step = 0.02\n",
    "\n",
    "# Values of 0 to 1\n",
    "argmax_start = 0.4\n",
    "argmax_stop = 0.7\n",
    "argmax_step = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c784ba08-ce42-4581-b34c-1cb317b9e3ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45679780-a6b3-4c5b-97d9-639ca3271b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define label mapping\n",
    "LABEL_MAPPING = {\"A\": 0, \"H\": 1, \"N\": 2, \"S\": 3}\n",
    "EPOCH = 1\n",
    "LEARNING_RATE = 0.00001\n",
    "EARLY_STOPPING = 2\n",
    "SEED = 2024\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACC_STEPS = 1 #disable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9e39b-1426-4180-99d3-987790003513",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data Augmentation Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf9e45-3e84-49d2-aa32-0480f65e6395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_CLASS_WEIGHTS = False # Leave false for higher score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c1d3f-dfb0-44e9-b51d-980a2b7c04e3",
   "metadata": {},
   "source": [
    "#### Run Gridsearch (Will stop notebook after params found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579deae2-dc65-41e8-9cc4-4aec9e31f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GRIDSEARCH = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba86e08-c177-4ada-9174-d618701d69a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fixed Paramters & Debug Flags\n",
    "***Please do not change anything within here!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988f206-670a-449b-9397-8096aed339e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_DEBUG_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfbdd5-6069-4035-a905-f106f87a70d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVE_FINAL_PREDICTION_CSV = True\n",
    "FORMAT_CSV_FOR_KAGGLE = True\n",
    "PRINT_ARGUMENTS = False\n",
    "SHOW_CM_AFTER_EACH_BATCH = False # Keep false for much clearer final output\n",
    "SAVE_FINAL_PREDICTION_CSV = True\n",
    "FORMAT_CSV_FOR_KAGGLE = True\n",
    "RUN_PARAMETER_SEARCH = True\n",
    "FORCE_SKIP_MULTIPREDICT = False\n",
    "FORCE_SKIP_PARAMTER_SEARCH = False\n",
    "\n",
    "if USE_DEBUG_DATASET:\n",
    "    enthropy_start = 1.1\n",
    "    enthropy_stop = 1.2\n",
    "    enthropy_step = 0.01\n",
    "    sentiment_start = 0.76\n",
    "    sentiment_stop = 0.8\n",
    "    sentiment_step = 0.02\n",
    "    argmax_start = 0.66\n",
    "    argmax_stop = 0.7\n",
    "    argmax_step = 0.002\n",
    "    \n",
    "if BEST_MODEL_MODE or USE_GRIDSEARCH:\n",
    "    DEFAULT_MODE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150b068-bcba-4880-a193-b3540b189306",
   "metadata": {},
   "source": [
    "#### NLP Preprocessing Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be37f0-3c28-4cb4-a9c3-0d86f3fd277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLY_NLP_PREPROCESS = True\n",
    "APPLY_CONTRACTIONS = True\n",
    "APPLY_LEMMANTIZATION = True\n",
    "REMOVE_STOPWORDS = False # Reduces overall accuracy. Leave off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015b5be-d0df-46ea-a349-ff312a02f02e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "SPEECH_TO_TEXT_MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "SENTIMENT_MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "TRAINING_OUTPUT_FOLDER = \"./Training-Output\"\n",
    "PREDICTION_OUTPUT_FOLDER = \"./Predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a4794-4a4e-40c9-8e06-b3da0f35b81a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cloud/Local Instance Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51e86f-e77a-4eee-b7bd-77d535a5f74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CLOUD: # Running on kaggle\n",
    "    TSV = CLOUD_TSV_LOCATION\n",
    "    AUDIO_DIRECTORY = CLOUD_AUDIO_DIRECTORY\n",
    "    REPORT_TO = 'none'\n",
    "    !pip install contractions\n",
    "    !pip install clean-text\n",
    "    !pip install nltk\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    \n",
    "    if USE_GRIDSEARCH:\n",
    "        !pip install optuna\n",
    "    \n",
    "else: # Running on local Jupyter instance\n",
    "    TSV = r'C:\\Users\\ChenYi\\Downloads\\AAI3001_Project\\labels\\IEMOCAP_4.tsv'\n",
    "    AUDIO_DIRECTORY = r'C:\\Users\\ChenYi\\Downloads\\AAI3001_Project\\small-project\\IEMOCAP_full_release_audio'\n",
    "    REPORT_TO = 'all'\n",
    "    NUM_WORKERS = 0 # Must set to zero to run\n",
    "    BATCH_SIZE = 8 # Adjust to fit model on VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738b868-2075-469d-ad1a-e10bc9526e36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93bb7c-ece0-4231-987a-6956f82b5717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import librosa\n",
    "import torchaudio\n",
    "import contractions\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torchaudio import functional as audioF\n",
    "from torchaudio.transforms import Resample\n",
    "from torchaudio.compliance import kaldi\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import EarlyStoppingCallback, AdamW, get_scheduler\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a7fe1-363b-4501-ab76-8ee40b22cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY_NLP_PREPROCESS:\n",
    "    import spacy\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    mapping = str.maketrans('', '', string.digits) # table to remove strings\n",
    "    \n",
    "if USE_GRIDSEARCH:\n",
    "    import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7c153-0c5e-4da3-b213-956d40b46995",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Provided Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe69002-c07d-4cc9-9eda-a6570dc0ca86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pad_trunc_wav(nn.Module):\n",
    "    def __init__(self, max_len: int = 6*16000):\n",
    "        super(Pad_trunc_wav, self).__init__()\n",
    "        self.max_len = max_len\n",
    "    def forward(self,x):\n",
    "        shape = x.shape\n",
    "        length = shape[1]\n",
    "        if length < self.max_len:\n",
    "            multiple = self.max_len//length+1\n",
    "            x_tmp = torch.cat((x,)*multiple, axis=1)\n",
    "            x_new = x_tmp[:,0:self.max_len]\n",
    "        else:\n",
    "            x_new = x[:,0:self.max_len]\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a865a3b-4301-4e6a-be68-3e97493fd6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed=2021):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "setup_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1f002-cc58-4634-80f2-5242e4aa3174",
   "metadata": {},
   "source": [
    "### Download Required Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52daf6-8559-477c-8e2d-55effc92fb55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_MODEL_NAME,\n",
    "    num_labels = 4)\n",
    "\n",
    "# Load Wav2Vec2 model and processor for speech-to-text\n",
    "S2T_processor = Wav2Vec2Processor.from_pretrained(SPEECH_TO_TEXT_MODEL_NAME)\n",
    "S2T_Model = Wav2Vec2ForCTC.from_pretrained(SPEECH_TO_TEXT_MODEL_NAME)\n",
    "S2T_Model = S2T_Model.to('cpu')\n",
    "\n",
    "# Load sentiment analysis model\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=SENTIMENT_MODEL_NAME, tokenizer=SENTIMENT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f0365-cf0c-4e39-b4df-c0504fde6b78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce564f9-585e-4b73-b529-aa6ff9a334e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    def __init__(self, mode='train', max_len=6, seed=42, data_path=TSV, audio_dir=AUDIO_DIRECTORY):\n",
    "        self.mode = mode\n",
    "        data_all = pd.read_csv(data_path, sep='\\t')\n",
    "        SpkNames = np.unique(data_all['speaker'])  # ['Ses01F', 'Ses01M', ..., 'Ses05M']\n",
    "        self.data_info = self.split_dataset(data_all, SpkNames)\n",
    "        self.get_audio_dir_path = os.path.join(audio_dir)\n",
    "        self.pad_trunc = Pad_trunc_wav(max_len * 16000)\n",
    "         \n",
    "        # Label encoding\n",
    "        self.label = self.data_info['label'].astype('category').cat.codes.values\n",
    "        self.ClassNames = np.unique(self.data_info['label'])\n",
    "        self.NumClasses = len(self.ClassNames)\n",
    "        if mode == 'train':\n",
    "            print(\"Each emotion has the following number of training samples:\")\n",
    "            print([[self.ClassNames[i], (self.label == i).sum()] for i in range(self.NumClasses)])\n",
    "        self.weight = 1 / torch.tensor([(self.label == i).sum() for i in range(self.NumClasses)]).float()\n",
    "\n",
    "    def get_classname(self):\n",
    "        return self.ClassNames\n",
    "\n",
    "    def split_dataset(self, df_all, speakers):\n",
    "        test_idx = df_all['speaker'] == speakers[0]  # 'Ses01F' as test set\n",
    "        val_idx = df_all['speaker'] == speakers[1]   # 'Ses01M' as validation set\n",
    "        train_idx = ~(test_idx | val_idx)             # Remaining speakers for training\n",
    "        train_data_info = df_all[train_idx].reset_index(drop=True)\n",
    "        val_data_info = df_all[val_idx].reset_index(drop=True)\n",
    "        test_data_info = df_all[test_idx].reset_index(drop=True)\n",
    "        debug_data_info = df_all[test_idx].reset_index(drop=True)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            data_info = train_data_info\n",
    "        elif self.mode == 'val':\n",
    "            data_info = val_data_info\n",
    "        elif self.mode == 'test':\n",
    "            data_info = test_data_info\n",
    "        elif self.mode == 'debug':\n",
    "            data_info = test_data_info.sample(n=20, random_state=42).reset_index(drop=True)\n",
    "        else:\n",
    "            data_info = df_all\n",
    "        return data_info\n",
    "\n",
    "    def pre_process(self, wav):\n",
    "        wav = self.pad_trunc(wav)\n",
    "        return wav\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the raw waveform from file using data_info to get filenames\n",
    "        wav_path = os.path.join(self.get_audio_dir_path, self.data_info['filename'][idx]) + '.wav'\n",
    "        wav, sample_rate = torchaudio.load(wav_path)\n",
    "\n",
    "        # Preprocess the waveform (e.g., pad/truncate if needed)\n",
    "        wav = self.pre_process(wav)\n",
    "\n",
    "        # Apply Wav2Vec2 feature extractor\n",
    "        inputs = feature_extractor(\n",
    "            wav.squeeze().numpy(),  # Convert PyTorch tensor to numpy array\n",
    "            sampling_rate=sample_rate,\n",
    "            return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "            padding=True  # Optionally pad to a fixed length\n",
    "        )\n",
    "\n",
    "        label = self.label[idx]\n",
    "\n",
    "        # Return the processed input values and the label\n",
    "        return {\n",
    "            'input_values': inputs['input_values'].squeeze(0),  # Remove extra batch dimension\n",
    "            'labels': torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f14513-26da-4c82-91b2-8cbf4e6a040b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate datasets\n",
    "train_dataset = Mydataset(mode='train', max_len=6)\n",
    "val_dataset = Mydataset(mode='val', max_len=6)\n",
    "test_dataset = Mydataset(mode='test', max_len=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c109f0-bf0f-408f-8e54-c02e9577d518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_DEBUG_DATASET: # Smaller dataset to run the entire notebook faster for debug\n",
    "    test_dataset = Mydataset(mode='debug', max_len=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42062d0d-e27c-4419-93a0-7ff5e8629f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put test information into a dataframe for later use\n",
    "data_info = test_dataset.data_info\n",
    "test_dataframe = data_info[['filename', 'label']].copy()\n",
    "test_dataframe['filepath'] = test_dataframe['filename'].apply(\n",
    "    lambda x: os.path.join(test_dataset.get_audio_dir_path, f\"{x}.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a3c03-3bd8-4f17-9cb0-13e3237ead7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f1997-55d5-471e-bdbb-e6baf7545ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLASS_WEIGHTS:\n",
    "    class WeightedTrainer(Trainer):\n",
    "        def __init__(self, class_weights, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "\n",
    "            # Move class_weights to the same device as the model\n",
    "            self.class_weights = class_weights.to(self.args.device)\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get(\"logits\")\n",
    "            # Define the loss function with class weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1f623-081f-4ee4-b9a4-0f7e5f672d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    accuracy = np.sum(preds == labels) / len(labels)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience = EARLY_STOPPING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95346986-62ae-4ab6-a04f-d2837decff58",
   "metadata": {},
   "source": [
    "#### Gridsearch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba77bff-e3c1-47e1-953a-f0e91565b9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_GRIDSEARCH:\n",
    "    def optuna_hp_space(trial):\n",
    "        return {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "            \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
    "        }\n",
    "    \n",
    "    TOTAL_TRIALS = 4 # Update n_trials here!\n",
    "    \n",
    "    def model_init(trial):\n",
    "        return Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            PRE_TRAINED_MODEL_NAME,\n",
    "            num_labels = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c2bbf-6692-4b74-a74b-a71832dd4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure arguments to be gridsearched are not listed below\n",
    "if USE_GRIDSEARCH:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= TRAINING_OUTPUT_FOLDER,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs = EPOCH,\n",
    "        save_steps = 10,\n",
    "        save_total_limit = 2,\n",
    "        logging_dir=\"./logs\",\n",
    "        fp16 = True,\n",
    "        dataloader_pin_memory = True,\n",
    "        load_best_model_at_end = True,\n",
    "        dataloader_num_workers = NUM_WORKERS,\n",
    "        report_to = REPORT_TO,\n",
    "        gradient_accumulation_steps = GRADIENT_ACC_STEPS,\n",
    "        gradient_checkpointing = True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=None,\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0ad2f-0236-4225-bd36-cf29c8b49ecd",
   "metadata": {},
   "source": [
    "#### Regular Model Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9c623-933e-454d-8a85-98cea48b0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GRIDSEARCH:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= TRAINING_OUTPUT_FOLDER,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        per_device_eval_batch_size = BATCH_SIZE,\n",
    "        num_train_epochs = EPOCH,\n",
    "        save_steps = 10,\n",
    "        save_total_limit = 2,\n",
    "        logging_dir=\"./logs\",\n",
    "        fp16 = True,\n",
    "        dataloader_pin_memory = True,\n",
    "        load_best_model_at_end = True,\n",
    "        dataloader_num_workers = NUM_WORKERS,\n",
    "        report_to = REPORT_TO,\n",
    "        gradient_accumulation_steps = GRADIENT_ACC_STEPS,\n",
    "        gradient_checkpointing = True,\n",
    "        learning_rate = LEARNING_RATE\n",
    "    )\n",
    "\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        class_weights = train_dataset.weight\n",
    "        trainer = WeightedTrainer(\n",
    "            class_weights=class_weights,\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[early_stopping]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a846f4-aebe-4a82-ba9c-e4e8ce4e31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_dataframe, model=''):\n",
    "    \n",
    "    results = []\n",
    "    total = test_dataframe.shape[0]\n",
    "    count = 1\n",
    "    \n",
    "    # Run predictions on test dataset\n",
    "    for index, row in test_dataframe.iterrows():\n",
    "\n",
    "        # Display progress\n",
    "        print(f'File {count} of {total}', end='\\r')\n",
    "        count += 1\n",
    "\n",
    "        # Load audio file\n",
    "        filename = row['filename'] + '.wav'\n",
    "        audio_file = os.path.join(AUDIO_DIRECTORY, filename)\n",
    "        y_ini, sr_ini = librosa.load(audio_file, sr = 16000)\n",
    "\n",
    "        inputs = feature_extractor(y_ini, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "        # Get the logits from the model\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        # Predict the class with the highest logit value\n",
    "        predicted_class_id = torch.argmax(logits).item()\n",
    "\n",
    "        # Append the result to the list\n",
    "        results.append([row['filename'], predicted_class_id])\n",
    "\n",
    "    # Format to dataframe\n",
    "    prediction_dataframe = pd.DataFrame(results, columns=['ID', 'Predict'])\n",
    "\n",
    "\n",
    "    # Load true values\n",
    "    true_dataframe = pd.read_csv(TSV, sep='\\t')\n",
    "    remap_dict = {\n",
    "        0: 'A',\n",
    "        1: 'H',\n",
    "        2: 'N',\n",
    "        3: 'S'}\n",
    "\n",
    "    # Remap predicted values to match TSV\n",
    "    prediction_dataframe['Predict'] = prediction_dataframe['Predict'].map(remap_dict)\n",
    "\n",
    "    # Merge DataFrames on 'filename'\n",
    "    df_merged = pd.merge(true_dataframe[['filename', 'label']],prediction_dataframe[['ID', 'Predict']],\n",
    "                         left_on='filename',right_on='ID')\n",
    "\n",
    "    # Extract true labels and predictions\n",
    "    y_true = df_merged['label']\n",
    "    y_pred = df_merged['Predict']\n",
    "    \n",
    "    # Compute and print UA score\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Test UA: {macro_recall}\")\n",
    "        \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Create a DataFrame for the confusion matrix\n",
    "    labels = sorted(y_true.unique())\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.ylabel('Actual Labels')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Compute and print classification report\n",
    "    report = classification_report(y_true, y_pred, labels=labels)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91433214-6f66-4649-b9bb-8887e4aa1b59",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start Training\n",
    "***Or load a previously trained model***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd95e83-bae8-4819-a60c-02fdc0bc41e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "if USE_GRIDSEARCH:\n",
    "    best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"ray\",\n",
    "    hp_space=ray_hp_space,\n",
    "    n_trials=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d597f-2697-4671-bc10-3d58a1606e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_GRIDSEARCH:\n",
    "    best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\", # Ensure minimize for defualt metric of val loss\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=TOTAL_TRIALS\n",
    ")\n",
    "    print(\"Best Hyperparameters Found:\")\n",
    "    for param, value in best_trial.hyperparameters.items():\n",
    "        print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a4d49-e762-436e-87bd-585178d12919",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GRIDSEARCH:\n",
    "    if DEFAULT_MODE:\n",
    "\n",
    "        # Set Flag\n",
    "        MODEL_IS_NOT_TRAINED = False\n",
    "\n",
    "        # Start Training\n",
    "        trainer.train()\n",
    "\n",
    "        # Save\n",
    "        output_filepath = TRAINING_OUTPUT_FOLDER + \"/\" + OUTPUT_MODEL_NAME\n",
    "        trainer.save_model(output_filepath)\n",
    "        feature_extractor.save_pretrained(output_filepath)\n",
    "        training_args = TrainingArguments(output_dir=output_filepath)\n",
    "\n",
    "    else: # User want to load a previously fine tuned model\n",
    "\n",
    "        # Set Flag\n",
    "        MODEL_IS_NOT_TRAINED = True #Loaded model does not have any training logs.\n",
    "\n",
    "        # Load model\n",
    "        model = Wav2Vec2ForSequenceClassification.from_pretrained(BEST_MODEL_PATH)\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(BEST_MODEL_PATH)\n",
    "        print(f\"Previously trained model loaded from: {BEST_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eee53f-5221-4c0f-ab27-61e2b2db0bcd",
   "metadata": {},
   "source": [
    "#### Show Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd5eb1-2ca5-4d32-87bf-b53eb074be26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not USE_GRIDSEARCH:\n",
    "    if PLOT_TRAINING_GRAPHS and not MODEL_IS_NOT_TRAINED:\n",
    "\n",
    "        train_losses = {}\n",
    "        val_losses = {}\n",
    "        epochs = set()\n",
    "\n",
    "        for log in trainer.state.log_history:\n",
    "            if 'epoch' in log:\n",
    "                epoch = log['epoch']\n",
    "                epochs.add(epoch)\n",
    "                if 'loss' in log:\n",
    "                    train_losses[epoch] = log['loss']\n",
    "                if 'eval_loss' in log:\n",
    "                    val_losses[epoch] = log['eval_loss']\n",
    "\n",
    "        # Sort the epochs and extract losses\n",
    "        epochs = sorted(epochs)\n",
    "        train_loss_list = [train_losses.get(epoch) for epoch in epochs]\n",
    "        val_loss_list = [val_losses.get(epoch) for epoch in epochs]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(epochs, train_loss_list, label='Training Loss')\n",
    "        plt.plot(epochs, val_loss_list, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss per Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6443ff-6b99-44bd-905f-6e08629c33f1",
   "metadata": {},
   "source": [
    "#### Print Model Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7178a-47a8-4c4d-a1f5-35cd4a3c4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GRIDSEARCH:\n",
    "    if PRINT_ARGUMENTS:\n",
    "        filepath = PREVIOUSLY_TRAINED_MODEL_PATH + \"/training_args.bin\"\n",
    "        training_args = torch.load(filepath)\n",
    "        print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224faceb-d11c-47cb-b02b-2604ccc6e4aa",
   "metadata": {},
   "source": [
    "#### Calculate Test Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a8afa-a060-474a-a9c1-bca043d86fbc",
   "metadata": {
    "id": "e30a8afa-a060-474a-a9c1-bca043d86fbc"
   },
   "outputs": [],
   "source": [
    "if not USE_GRIDSEARCH:\n",
    "    model = model.to('cpu')\n",
    "    test_model(test_dataframe, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f0f94-dcab-420a-a947-3ecbda308f69",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Multi Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999de92-ffe1-4952-9842-6ab1315024ba",
   "metadata": {},
   "source": [
    "#### Clean and preprocess text for sentiment analysis\n",
    "    * Expanding contractions\n",
    "    * Removing punctuations\n",
    "    * Lemmatizing text\n",
    "    * Lowercasing\n",
    "    * Remove Numbers\n",
    "    * Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49285db-3d8b-4d7f-8190-aa701cb60cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_Preprocess(string):\n",
    "     \n",
    "    output = string\n",
    "    \n",
    "    if APPLY_CONTRACTIONS:\n",
    "    \n",
    "        # Expand Contractions\n",
    "        words = string.split()\n",
    "        output = [contractions.fix(word) for word in words]\n",
    "        output = ' '.join(output)\n",
    "        \n",
    "    if APPLY_LEMMANTIZATION:\n",
    "        doc = nlp(string)\n",
    "        output = \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "    # Remove Numbers\n",
    "    output = output.translate(mapping)\n",
    "    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        doc = nlp(string)\n",
    "        output = [token.text for token in doc if not token.is_stop]\n",
    "        output= ' '.join(output)\n",
    "        \n",
    "    return output.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea8706-0871-42b3-81b7-7b9c9fd298ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_predict(model, test_dataframe):\n",
    "\n",
    "    results = []\n",
    "    total = test_dataframe.shape[0]\n",
    "    count = 1\n",
    "\n",
    "    # Iterate over each audio file in the test folder\n",
    "    for index, row in test_dataframe.iterrows():\n",
    "\n",
    "        # Display progress\n",
    "        print(f'File {count} of {total}', end='\\r')\n",
    "        count += 1\n",
    "\n",
    "        # Load audio file\n",
    "        filename = row['filename'] + '.wav'\n",
    "        audio_file = os.path.join(AUDIO_DIRECTORY, filename)\n",
    "        audio, sample_rate = librosa.load(audio_file, sr = 16000)\n",
    "\n",
    "        inputs = feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        \n",
    "        # Tokenize the input audio for speech-to-text model\n",
    "        input_values = S2T_processor(audio, return_tensors=\"pt\", sampling_rate=16000, padding=\"longest\").input_values\n",
    "\n",
    "        # Extract features from the audio\n",
    "        inputs = feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "        # Get the logits from the model\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            S2T_logits = S2T_Model(input_values).logits\n",
    "            \n",
    "            # Retrieve logits and decode the predicted ids for transcription\n",
    "            predicted_ids = torch.argmax(S2T_logits, dim=-1)\n",
    "            transcription = S2T_processor.batch_decode(predicted_ids)[0]\n",
    "            \n",
    "            if APPLY_NLP_PREPROCESS:\n",
    "                transcription = NLP_Preprocess(transcription)\n",
    "\n",
    "        # Predict the class with the highest logit value\n",
    "        predicted_class_id = torch.argmax(logits).item()\n",
    "        \n",
    "        # Apply softmax to logits to get the probabilities\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Run sentiment analysis on the transcription\n",
    "        sentiment = sentiment_task(transcription)\n",
    "        sentiment_label = sentiment[0]['label']\n",
    "        sentiment_score = sentiment[0]['score']\n",
    "\n",
    "        # Extract the filename without the extension\n",
    "        filename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "\n",
    "        # Append the result to the list\n",
    "        results.append([filename, predicted_class_id, probabilities[0][0].item(), probabilities[0][1].item(), \n",
    "                        probabilities[0][2].item(), probabilities[0][3].item(), sentiment_label, sentiment_score])\n",
    "\n",
    "    # Write the results to a CSV file\n",
    "    global MULTI_PREDICTION_CSV_FILEPATH\n",
    "    MULTI_PREDICTION_CSV_FILEPATH = PREDICTION_OUTPUT_FOLDER + \"/\" + OUTPUT_MODEL_NAME + \"-raw.csv\"\n",
    "    os.makedirs(PREDICTION_OUTPUT_FOLDER, exist_ok=True)\n",
    "    \n",
    "    with open(MULTI_PREDICTION_CSV_FILEPATH , 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['ID', 'Predict', '0_Score','1_Score', '2_Score', '3_Score', 'Sentiment', 'Sentiment_Score'])\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"Multi model predictions saved to {MULTI_PREDICTION_CSV_FILEPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd6b78-d5ee-4ffd-97e0-a81a4223bd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not FORCE_SKIP_MULTIPREDICT and not USE_GRIDSEARCH:\n",
    "        model = model.to('cpu')\n",
    "        multi_predict(model, test_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d8c7a-a9b4-4783-9435-bc3d0d7c979d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mutli Model Prediction Merging Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cf83d-955a-4da6-bb59-6ee05638e2fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278cb0e-2792-4537-ba4d-2e0b0602a7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate entropy function\n",
    "def calculate_entropy(w, x, y ,z):\n",
    "    probabilities = torch.tensor([w, x, y, z])\n",
    "    entropy = -torch.sum(probabilities* torch.log(probabilities + 1e-10)) # small value to avoid log(0)\n",
    "    return entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb204ac-ffcf-4581-bffd-de145d532ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remaps the 3 sentiment to the 4 emotions.\n",
    "def remap_sentiment(mode, dataframe_row, sentiment_threshold, flip):\n",
    "    \n",
    "    sentiment = dataframe_row['Sentiment']\n",
    "    score = dataframe_row['Sentiment_Score']\n",
    "    \n",
    "    if sentiment == 'neutral':\n",
    "        return 2\n",
    "    elif sentiment == 'positive':\n",
    "        return 1\n",
    "    else: # Else sentiment is negative\n",
    "        \n",
    "        if mode == 'simple': \n",
    "            # There can be two possible mapping for negative sentiment.\n",
    "            # Either to sad or angry.\n",
    "            if score <= sentiment_threshold:\n",
    "                return 0 if not flip else 3\n",
    "            else:\n",
    "                return 3 if not flip else 0\n",
    "        \n",
    "        # If sentiment is negative, refer to orignal model to decidce how to map\n",
    "        # negative sentiment to both angry or sad\n",
    "        else:\n",
    "            angry_score = dataframe_row['0_Score']\n",
    "            sad_score = dataframe_row['3_Score']\n",
    "            if angry_score >= sad_score:\n",
    "                return 0\n",
    "            else:\n",
    "                return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48d257-c256-4fe6-becd-d06a8ec4a921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to perform accuracy evaluation\n",
    "def calculate_accuracy(dataframe, mode, show, save_file):\n",
    "    \n",
    "    # Define a remapping dictionary\n",
    "    remap_dict = {\n",
    "        0: 'A',\n",
    "        1: 'H',\n",
    "        2: 'N',\n",
    "        3: 'S'\n",
    "    }\n",
    "    \n",
    "    if mode: # Apply the remap to the original model's predictions\n",
    "        \n",
    "        # Apply the remap function to the 'Predict' column\n",
    "        dataframe['Predict'] = dataframe['Predict'].map(remap_dict)\n",
    "\n",
    "        # Merge DataFrames on 'filename'\n",
    "        df_merged = pd.merge(reference[['filename', 'label']],dataframe[['ID', 'Predict']],left_on='filename',right_on='ID')\n",
    "\n",
    "        # Extract true labels and predictions\n",
    "        y_true = df_merged['label']\n",
    "        y_pred = df_merged['Predict']\n",
    "        \n",
    "    else: # Apply the remap to the modified final predictions\n",
    "        \n",
    "        if save_file and FORMAT_CSV_FOR_KAGGLE:\n",
    "            kaggle_dataframe = pd.DataFrame()\n",
    "            kaggle_dataframe['ID'] = dataframe['ID']\n",
    "            kaggle_dataframe['Predict'] = dataframe['Final']\n",
    "        \n",
    "        # Apply the remap function to the 'Predict' column\n",
    "        dataframe['Final'] = dataframe['Final'].map(remap_dict)\n",
    "\n",
    "        # Merge DataFrames on 'filename'\n",
    "        df_merged = pd.merge(reference[['filename', 'label']], dataframe[['ID', 'Final']],left_on='filename',right_on='ID')\n",
    "\n",
    "        # Extract true labels and predictions\n",
    "        y_true = df_merged['label']\n",
    "        y_pred = df_merged['Final']\n",
    "        \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Create a DataFrame for the confusion matrix\n",
    "    labels = sorted(y_true.unique())\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "    if show and SHOW_CM_AFTER_EACH_BATCH or FINAL_RUN:\n",
    "        # Plot the confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.ylabel('Actual Labels')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    # Compute and print UA score\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    if show and SHOW_CM_AFTER_EACH_BATCH or FINAL_RUN:\n",
    "        print(f\"test UA: {macro_recall}\")\n",
    "    \n",
    "        # Print the confusion matrix\n",
    "        #print(\"Confusion Matrix:\")\n",
    "        #print(cm_df)\n",
    "\n",
    "    if show and SHOW_CM_AFTER_EACH_BATCH or FINAL_RUN:\n",
    "        # Compute and print classification report\n",
    "        report = classification_report(y_true, y_pred, labels=labels)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        \n",
    "    if save_file:\n",
    "        \n",
    "        os.makedirs(PREDICTION_OUTPUT_FOLDER, exist_ok=True)\n",
    "        \n",
    "        if FORMAT_CSV_FOR_KAGGLE:          \n",
    "            csv_name = PREDICTION_OUTPUT_FOLDER + \"/label.csv\"\n",
    "            kaggle_dataframe.to_csv(csv_name, index=False)\n",
    "            print(f\"Predictions formatted and saved for kaggle submission at {csv_name}\")\n",
    "            \n",
    "        else:        \n",
    "            csv_name = PREDICTION_OUTPUT_FOLDER + \"/\" + OUTPUT_MODEL_NAME + \"-detailed.csv\"\n",
    "            df_merge_all = pd.merge(reference, dataframe ,left_on='filename',right_on='ID')\n",
    "            reorganize_column = df_merge_all['label'] # Shift correct labels for easier viewing later\n",
    "            df_merge_all = df_merge_all.drop(columns=['ID', 'label', 'duration', 'fold', 'condition'])\n",
    "            df_merge_all['label'] = reorganize_column\n",
    "            df_merge_all.to_csv(csv_name, index=False)\n",
    "            print(f\"Predictions saved for analysis at {csv_name}\")\n",
    "            \n",
    "    return macro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7234e-94c1-43f1-89e0-ed47984a92f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_best_results(use_entrophy, best_sentiment_threshold, best_entrophy_or_argmax_threshold, \n",
    "                      prediction_strategy, sentiment_strategy, sad_angry_flip, final_run):\n",
    "    \n",
    "    # Read back from disk a fresh state of dataframe\n",
    "    df = pd.read_csv(MULTI_PREDICTION_CSV_FILEPATH)\n",
    "\n",
    "    # Creates a column for the final prediction\n",
    "    df['Final'] = None\n",
    "    \n",
    "    # Iterate over each row in dataframe\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        if use_entrophy: # Calculate the entrophy in each row                           \n",
    "            apply_entrophy(df, index, row, best_entrophy_or_argmax_threshold, best_sentiment_threshold, sad_angry_flip, \n",
    "                           prediction_strategy, sentiment_strategy)        \n",
    "\n",
    "        else: # Use argmax for threshold                 \n",
    "            apply_argmax(df, index, row, best_entrophy_or_argmax_threshold, best_sentiment_threshold, sad_angry_flip,\n",
    "                         prediction_strategy, sentiment_strategy)\n",
    "            \n",
    "    if final_run:\n",
    "        df_save = df.copy()\n",
    "        df_original = df.copy()\n",
    "    \n",
    "    print(\"[Original Model] \", round(calculate_accuracy(df, True, True, False),4))\n",
    "    print(\"[Multi Model]    \", round(calculate_accuracy(df, False, True, False),4), \"\\n\")\n",
    "   \n",
    "    \n",
    "    if final_run and SAVE_FINAL_PREDICTION_CSV:\n",
    "        global FINAL_RUN\n",
    "        FINAL_RUN = True\n",
    "        calculate_accuracy(df_original , True, True, False)\n",
    "        calculate_accuracy(df_save, False, False, True)\n",
    "        #calculate_accuracy(df_save, False, False, True)\n",
    "        \n",
    "    if not final_run:\n",
    "        return [use_entrophy, best_sentiment_threshold, best_entrophy_or_argmax_threshold, prediction_strategy, sentiment_strategy, sad_angry_flip]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf5873-135e-4ea8-8397-ea4c9184229e",
   "metadata": {},
   "source": [
    "#### Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7a939-6fd5-45a5-8f68-7ed3e9b4baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_entrophy(df, index, row, entrophy_threshold, sentiment_threshold, sad_angry_flip, \n",
    "                   prediction_strategy = 'default', sentiment_strategy = 'simple'):\n",
    "    \n",
    "    entropy = calculate_entropy(row['0_Score'], row['1_Score'], row['2_Score'], row['3_Score'])\n",
    "    \n",
    "    # Threshold reached, apply additional logic to output\n",
    "    if entropy >= entrophy_threshold:\n",
    "        \n",
    "        # Default mode. 2nd model overrides original model\n",
    "        if prediction_strategy == 'default':\n",
    "            df.at[index, 'Final'] = remap_sentiment(sentiment_strategy, row, sentiment_threshold, sad_angry_flip)\n",
    "            \n",
    "        # original model seems to be good at detecting angry and happy. So even if threshold reached, we ignore\n",
    "        elif prediction_strategy == 'ignore':\n",
    "            df.at[index, 'Final'] = row['Predict']\n",
    "            \n",
    "        # If both models predict the same emotion, ignore any thresholds and assume it is correct \n",
    "        elif prediction_strategy == 'ignore-when-match':\n",
    "            \n",
    "            # Obtain remapped sentiment\n",
    "            sentiment = remap_sentiment(sentiment_strategy, row, sentiment_threshold, sad_angry_flip)\n",
    "            \n",
    "            # If both models agree on the same emotion\n",
    "            if sentiment == row['Predict']:\n",
    "                \n",
    "                # Copy over with no change\n",
    "                df.at[index, 'Final'] = row['Predict']\n",
    "                \n",
    "            else: # Prefer model 2 over original model\n",
    "                df.at[index, 'Final'] = sentiment\n",
    "            \n",
    "    \n",
    "    # Threshold not reached, copy original prediction to output column\n",
    "    else:\n",
    "        df.at[index, 'Final'] = row['Predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6eaf16-a686-4d2b-a301-792b96e4b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_argmax(df, index, row, argmax_threshold, sentiment_threshold, sad_angry_flip,\n",
    "                 prediction_strategy = 'default', sentiment_strategy = 'simple'):\n",
    "    \n",
    "    argmax_value = max(row['0_Score'], row['1_Score'], row['2_Score'], row['3_Score'])\n",
    "    \n",
    "    # Threshold reached, apply additional logic to output\n",
    "    if argmax_value <= argmax_threshold:\n",
    "        \n",
    "        # Default mode. 2nd model overrides original model\n",
    "        if prediction_strategy == 'default':\n",
    "            df.at[index, 'Final'] = remap_sentiment(sentiment_strategy, row, sentiment_threshold, sad_angry_flip)\n",
    "            \n",
    "        # original model seems to be good at detecting angry and happy. So even if threshold reached, we ignore\n",
    "        elif prediction_strategy == 'ignore':\n",
    "            df.at[index, 'Final'] = row['Predict']\n",
    "            \n",
    "        # If both models predict the same emotion, ignore any thresholds and assume it is correct \n",
    "        elif prediction_strategy == 'ignore-when-match':\n",
    "            \n",
    "            # Obtain remapped sentiment\n",
    "            sentiment = remap_sentiment(sentiment_strategy, row, sentiment_threshold, sad_angry_flip)\n",
    "            \n",
    "            # If both models agree on the same emotion\n",
    "            if sentiment == row['Predict']:\n",
    "                \n",
    "                # Copy over with no change\n",
    "                df.at[index, 'Final'] = row['Predict']\n",
    "                \n",
    "            else: # Prefer model 2 over original model\n",
    "                df.at[index, 'Final'] = sentiment\n",
    "            \n",
    "    \n",
    "    # Threshold not reached, copy original prediction to output column\n",
    "    else:\n",
    "        df.at[index, 'Final'] = row['Predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9b221-0ca5-4695-beaf-23ff55b48dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MergePrediction(use_entrophy, sentiment_threshold_range, entrophy_threshold_range, prediction_strategy, sentiment_strategy, sad_angry_flip):\n",
    "    \n",
    "     # keeps track of progress\n",
    "    iter_counter = 1\n",
    "    global total_batches, current_batch\n",
    "    \n",
    "    # Hold best params\n",
    "    best_difference = 0\n",
    "    best_sentiment_threshold = 0\n",
    "    best_entrophy_or_argmax_threshold = 0\n",
    "    best_flip = False\n",
    "    \n",
    "    # Use user selected threshold metric\n",
    "    if use_entrophy:\n",
    "        threshold_type = entrophy_threshold_range\n",
    "        total_iter = len(entrophy_threshold_range) * len(sentiment_threshold_range)\n",
    "    else:\n",
    "        threshold_type = argmax_threshold_range\n",
    "        total_iter = len(argmax_threshold_range) * len(sentiment_threshold_range)\n",
    "    \n",
    "    for entro_or_argmax in threshold_type:\n",
    "        for senti in sentiment_threshold_range:\n",
    "            \n",
    "            print(f\"Iteration {iter_counter} of {total_iter} [Batch {current_batch} of {total_batches}]\", end='\\r')\n",
    "            iter_counter += 1\n",
    "\n",
    "            entrophy_or_argmax_threshold = round(entro_or_argmax,3)\n",
    "            sentiment_threshold = round(senti,3)\n",
    "        \n",
    "            # Read back from disk a fresh state of the dataframe\n",
    "            df = pd.read_csv(MULTI_PREDICTION_CSV_FILEPATH)\n",
    "            \n",
    "            # Creates a column for the final output prediction\n",
    "            df['Final'] = None\n",
    "       \n",
    "            # Iterate over each row in dataframe\n",
    "            for index, row in df.iterrows():\n",
    "                \n",
    "                if use_entrophy: # Calculate the entrophy in each row                           \n",
    "                    apply_entrophy(df, index, row, entrophy_or_argmax_threshold, sentiment_threshold, sad_angry_flip, \n",
    "                                   prediction_strategy, sentiment_strategy)        \n",
    "                    \n",
    "                else: # Use argmax for threshold                 \n",
    "                    apply_argmax(df, index, row, entrophy_or_argmax_threshold, sentiment_threshold, sad_angry_flip,\n",
    "                                  prediction_strategy, sentiment_strategy)\n",
    "                    \n",
    "                 \n",
    "            # Start before and after comparison\n",
    "            original_accuracy = calculate_accuracy(df, True, False, False)           \n",
    "            new_accuracy = calculate_accuracy(df, False, False, False)                \n",
    "            difference = new_accuracy - original_accuracy\n",
    "      \n",
    "            # Check if current run has the best results\n",
    "            if difference > best_difference:\n",
    "                best_difference = difference\n",
    "                best_sentiment_threshold = sentiment_threshold\n",
    "                best_entrophy_or_argmax_threshold = entrophy_or_argmax_threshold\n",
    "                best_flip = sad_angry_flip\n",
    "                \n",
    "    \n",
    "    current_batch += 1\n",
    "\n",
    "    #Print out best params at the end\n",
    "    if best_difference != 0:\n",
    "        \n",
    "        print(\"-------- Run Completed Report --------\")\n",
    "        if use_entrophy:        \n",
    "            print(\"Metric Used: Entrophy\")\n",
    "            print(f\"Prediction Strategy Used: {prediction_strategy}\")\n",
    "            print(f\"Sentiment Strategy Used: {sentiment_strategy}\")\n",
    "            print(f\"Best entrophy_threshold is {best_entrophy_or_argmax_threshold}\")\n",
    "        else:\n",
    "            print(\"Metric Used: Argmax\")\n",
    "            print(f\"Strategy Used: {prediction_strategy}\")\n",
    "            print(f\"Sentiment Strategy Used: {sentiment_strategy}\")\n",
    "            print(f\"Best best_argmax_threshold is {best_entrophy_or_argmax_threshold}\")\n",
    "        print(f\"Best sentiment_threshold is {best_sentiment_threshold}\")\n",
    "        print(f\"And sad_angry_flip set to: {best_flip}\")\n",
    "        print(f\"with an improvement of {best_difference:.4f}\\n\")\n",
    "        \n",
    "        best_params_list = show_best_results(use_entrophy, best_sentiment_threshold, best_entrophy_or_argmax_threshold,\n",
    "                                             prediction_strategy, sentiment_strategy, best_flip, False)\n",
    "        best_params_list.append(\"{:.4g}\".format(best_difference))\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nWent through {iter_counter-1} iterations\")\n",
    "        print(\"Multi Model did not improve overall results\\n\")\n",
    "        \n",
    "        best_params_list = [None, None, None, prediction_strategy, False, \"0\"]  # Default placeholders\n",
    " \n",
    "    return best_params_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ba0cf-4da6-4672-bd27-97564ba598bc",
   "metadata": {},
   "source": [
    "#### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db908c04-440e-4662-b02c-9591ca301e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not FORCE_SKIP_PARAMTER_SEARCH and not USE_GRIDSEARCH:\n",
    "\n",
    "    overall_best_metric = ''\n",
    "    overall_best_prediction_strategy = ''\n",
    "    overall_best_sentiment_strategy = ''\n",
    "    overall_best_metric_threshold = 0.0\n",
    "    overall_best_sentiment_threshold = 0.0\n",
    "    overall_best_improvement = 0.0\n",
    "    overall_best_flip = False\n",
    "\n",
    "    FINAL_RUN = False\n",
    "\n",
    "    sentiment_threshold_range = np.arange(sentiment_start , sentiment_stop + sentiment_step , sentiment_step)\n",
    "    entrophy_threshold_range = np.arange(enthropy_start , enthropy_stop + enthropy_step , enthropy_step)\n",
    "    argmax_threshold_range = np.arange(argmax_start , argmax_stop + argmax_step , argmax_step)\n",
    "    reference = pd.read_csv(TSV, sep='\\t')\n",
    "\n",
    "    entrophy_options = [True, False]\n",
    "    flip_options = [True, False]\n",
    "    prediction_options = ['ignore-when-match', 'ignore']\n",
    "    sentiment_options = ['simple', 'refer']\n",
    "\n",
    "\n",
    "    total_batches = (len(entrophy_options) * len(flip_options) * len(prediction_options) * len(sentiment_options)) \n",
    "    current_batch = 1\n",
    "\n",
    "    list_of_results = []\n",
    "\n",
    "    for use_entrophy in entrophy_options:\n",
    "        for sad_angry_flip in flip_options:\n",
    "            for prediction_strategy in prediction_options:\n",
    "                for sentiment_strategy in sentiment_options:\n",
    "\n",
    "                    # Run with the current parameter combination\n",
    "                    result = MergePrediction(use_entrophy, sentiment_threshold_range, entrophy_threshold_range, \n",
    "                                             prediction_strategy, sentiment_strategy, sad_angry_flip)\n",
    "                    list_of_results.append(result)\n",
    "\n",
    "                    current_metric = result[0]\n",
    "                    current_improvement = float(result[-1])\n",
    "\n",
    "                    # Check if the current result is better than the overall best\n",
    "                    if current_improvement > overall_best_improvement:\n",
    "\n",
    "                        # Update the overall best values\n",
    "                        overall_best_metric = current_metric\n",
    "                        overall_best_prediction_strategy = prediction_strategy\n",
    "                        overall_best_sentiment_strategy = sentiment_strategy\n",
    "                        overall_best_metric_threshold = result[2]\n",
    "                        overall_best_sentiment_threshold = result[1]\n",
    "                        overall_best_improvement = current_improvement\n",
    "                        overall_best_flip = result[-2]\n",
    "\n",
    "    # Print the best result\n",
    "    if overall_best_metric:\n",
    "        text = 'Entrophy'\n",
    "        use_entro = True\n",
    "    else:\n",
    "        text = 'Argmax'\n",
    "        use_entro = False\n",
    "    print(\"-------------------------------------- Overall Run Report --------------------------------------\")\n",
    "    print(f\"Best overall improvement of: +{overall_best_improvement}\")\n",
    "    print(f\"Prediction Strategy: '{overall_best_prediction_strategy}' \")\n",
    "    print(f\"Sentiment Strategy: '{overall_best_sentiment_strategy}' \")\n",
    "    print(f\"Metric: '{text}' with threshold of {overall_best_metric_threshold}\")\n",
    "    print(f\"Sentiment threshold of {overall_best_sentiment_threshold}\")\n",
    "    print(f\"Flip: {overall_best_flip}\")\n",
    "    print(\"\\n------------------------------------- Overall Best Results -------------------------------------\")\n",
    "    show_best_results(use_entro, overall_best_sentiment_threshold, overall_best_metric_threshold, overall_best_prediction_strategy, overall_best_sentiment_strategy, overall_best_flip, True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AAI3001",
   "language": "python",
   "name": "aai3001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
