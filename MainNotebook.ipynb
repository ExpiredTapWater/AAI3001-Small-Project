{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36c34af-36e8-483f-90dd-bc10634ebbd2",
   "metadata": {
    "id": "e36c34af-36e8-483f-90dd-bc10634ebbd2",
    "tags": []
   },
   "source": [
    "## AAI30001 Small Project\n",
    "#### **Group: SP_8**\n",
    " - Chua Chen Yi (2302822)\n",
    " - Wong Jun Jai (2302765)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee5ec1-a889-4910-a57a-014952aabfb8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318d2a8-30c2-4751-81b4-04d809b39d35",
   "metadata": {
    "id": "1318d2a8-30c2-4751-81b4-04d809b39d35",
    "tags": []
   },
   "source": [
    "#### Our proposed method of improving accuracy over the baseline score is as follows:\n",
    "### 1. Identify a pre-trained model to be our baseline for fine-tuning\n",
    "- Before starting our search, we manually re-created the same testing environment as the sample code. This included splitting the dataset into training, validation and testing identically to the sample. In addition, the use of the TestUA for determining overall performance was use\n",
    "    - *Minor note: We have noticed the way the dataset is split for testing is not the same as how it is described in the text. For example, the test set should only contain files from 'Ses01F' and only from the female speaker. However, checking the dataset class showed that this is not true.*\n",
    "- With a simple evaluation pipeline in placee, we randomly choose 10 different models publically availble from hugging face, and proceeded to score them. They scored a range from 0.60-0.80 on the test dataset.\n",
    "- We decided to use the same base model: **\"facebook/wav2vec2-base\"** with the goal of fine tuning according to the rules and achieve at least 0.70\n",
    "### 2. Perform Data Augmentation On Dataset\n",
    "- #### Augmentation 1\n",
    "    - Details\n",
    "- #### Augmentation 2\n",
    "    - Details\n",
    "### 3. Identify Strengths & Weakness of Pre-Trained Model\n",
    "- We started by training the model for a few epochs to get a general idea of how it performs. A sample confusion matrix is shown here:\n",
    "\n",
    "             A   H    N   S\n",
    "        A  103   8   35   1\n",
    "        H    8  79   36   9\n",
    "        N    2   7  109  53 <- Highest Error\n",
    "        S    1   8   10  59\n",
    "        \n",
    "    In general, the model is able to differentiate angry and happy emotions with a high degree of accuracy. However, the model is not good at differentiaing between neutral and sad emotions. Its greatest weakness is predicting a neutral emotion as a sad one\n",
    "                \n",
    "### 4. Use Secondary Model With Text Embeddings\n",
    "- We have decided not to use extracted text embeddings as a feature of our first model, but instead have a completely seperate model extract and perform sentiment analysis on the text. The final prediction will be a combination of both models.\n",
    "    - This method allows us to:\n",
    "        1. Manage our time better as work can be done to improve the performance of both indepenently.\n",
    "        2. Change our base model if we find a better one.\n",
    "        3. Choose more sophisticated speech-to-text and sentiment analysis models\n",
    "### 5. Develop Algorithm To Merge Predictions\n",
    "- Our final implementation consist of getting the confidence for each label using softmax in addition to its original prediction. The predictions for the 2nd model is then merged into a single CSV file. An gridsearch-like function will identify the optimal parameters *(highlighted in **Bold**)*.\n",
    "- All models tested have improved scores. In general, models with <0.65 score will see a boost of 2-4%, while models >0.65 score will gain 0-2%. \n",
    "    ### Merge Algorithm\n",
    "    - The algorithm that determines the final predictions is a combination of 3 different strategies:\n",
    "        1. **Merging Strategy:** When to rely on the 2nd model?\n",
    "        2. **Prediction Strategy:** How to rely on the 2nd model?\n",
    "        3. **Mapping Strategy:** How to map sentiment to emotions?\n",
    "    ### 5A. Merging Strategy\n",
    "    We have identified 2 possible metrics to decide when to rely on the 2nd model\n",
    "    - **Entrophy Threshold**\n",
    "    We apply a calculate the entrophy based on the 4 confidence scores, following the logic that a lower overall entrophy will mean the model is most confident in its prediction. We will refer to the 2nd model when the entrophy is above the ***entrophy_threshold***.\n",
    "    - **Argmax Threshold**\n",
    "    We apply a simple argmax on the 4 confidence scores. If the value is below the ***argmax_threshold***, we will refer to the 2nd model for the final prediction\n",
    "    ### 5B. Prediction Strategy\n",
    "    We have identified 3 possible metrics to decide how to rely on the 2nd model\n",
    "    - **Default**\n",
    "    Prefer prediction of 2nd model in all situations\n",
    "    - **Ignore**\n",
    "    We identified the original model is very good at angry and happy emotions. So we always prefer the orignal model's predictions if it detects angry or happy\n",
    "    - **Ignore When Match**\n",
    "    If both models agree on the same prediction, we ignore however low the confidence is and assume is correct\n",
    "    ### 5C. Mapping Strategy\n",
    "    Because our sentiment analysis outputs 3 classes, while we have 4 emotions, we ill need to map 1 class to 2 emotions. This corresponds to the 'Negative' sentiment being mapped to either 'Angry' or 'Sad'. We have implemented the following methods:\n",
    "    - **Simple Mapping**\n",
    "    We decide on a ***sentiment_threshold*** value, where anything above is 'Sad'. This can also ***fliped*** around, ie. anything above is 'Angry' as there is no defined way to map the negative sentiment\n",
    "    - **Reference Mapping**\n",
    "    When a negative sentiment needs to be mapped, the confidence of 'Sad' and 'Angry' from the original model is looked up, and Argmax is used to return the most likely emotion.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73505ae0-7b64-4e97-94d6-1abb0d66e29d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## All Notebook Settings & Parameters\n",
    "***Everything that can be set and changed will be in here***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c1166-fece-43fe-86e7-ecf69b6ab1ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcfbdd5-6069-4035-a905-f106f87a70d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Colab/Kaggle Notebook.\n",
    "CLOUD = False #True for local instances\n",
    "\n",
    "# Model Loading and Output\n",
    "LOAD_PREVIOUSLY_TRAINED_MODEL = True\n",
    "PREVIOUSLY_TRAINED_MODEL_PATH = r'C:\\Users\\ChenYi\\Downloads\\AAI3001_Project\\TransferLearning\\models\\wav2vec2-large-e5'\n",
    "PRE_TRAINED_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "SPEECH_TO_TEXT_MODEL_NAME = \"\"\n",
    "SENTIMENT_MODEL_NAME = \"\"\n",
    "OUTPUT_MODEL_NAME = \"integrated-notebook-test-model\"\n",
    "OUTPUT_FOLDER = \"./Training-Output\"\n",
    "\n",
    "# Output Selection\n",
    "FORMAT_CSV_FOR_KAGGLE = False\n",
    "\n",
    "# Training Selction\n",
    "ONLY_RUN_TRAINING_LOOP = False # Runs the entire notebook. False stops the notebook once Test UA is computed\n",
    "JUST_PRINT_TEST_UA = True      # Prints only the score. False prints out all other metrics\n",
    "PLOT_TRAINING_GRAPHS = True    # Prints out train and val loss charts\n",
    "\n",
    "# If you already have a label.csv, can skip predict and directly calculate Test UA\n",
    "SKIP_PREDICT_AFTER_TRAINING = False\n",
    "\n",
    "# You can also skip prediction and directly calculate Test UA for a given label.csv\n",
    "BYPASS_MODEL_PREDICTION = False\n",
    "PREDICTION_CSV_FILEPATH = '...csv' #\n",
    "\n",
    "# Prediction Settings\n",
    "USE_MULTI_MODEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a4794-4a4e-40c9-8e06-b3da0f35b81a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cloud/Local Instance Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e51e86f-e77a-4eee-b7bd-77d535a5f74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CLOUD: # Running on kaggle\n",
    "    TSV = \"/kaggle/input/......\"\n",
    "    AUDIO_DIRECTORY = '/kaggle/input/.....'\n",
    "    REPORT_TO = 'none'\n",
    "    NUM_WORKERS = 4\n",
    "    BATCH = 32\n",
    "    \n",
    "else: # Running on local Jupyter instance\n",
    "    TSV = r'C:\\Users\\ChenYi\\Downloads\\AAI3001_Project\\labels\\IEMOCAP_4.tsv'\n",
    "    AUDIO_DIRECTORY = r'C:\\Users\\ChenYi\\Downloads\\AAI3001_Project\\small-project\\IEMOCAP_full_release_audio'\n",
    "    REPORT_TO = 'all'\n",
    "    NUM_WORKERS = 0 # Must set to zero to run\n",
    "    BATCH = 8 # Adjust to fit model on VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c784ba08-ce42-4581-b34c-1cb317b9e3ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45679780-a6b3-4c5b-97d9-639ca3271b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define label mapping\n",
    "LABEL_MAPPING = {\"A\": 0, \"H\": 1, \"N\": 2, \"S\": 3}\n",
    "#MAX_LEN = 6\n",
    "EPOCH = 2\n",
    "LEARNING_RATE = 0.00001\n",
    "EARLY_STOPPING = 10\n",
    "SEED = 2024\n",
    "GRADIENT_ACC_STEPS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738b868-2075-469d-ad1a-e10bc9526e36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e93bb7c-ece0-4231-987a-6956f82b5717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import librosa\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torchaudio import functional as audioF\n",
    "from torchaudio.transforms import Resample\n",
    "from torchaudio.compliance import kaldi\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import logging as log\n",
    "from transformers import EarlyStoppingCallback, AdamW, get_scheduler\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, accuracy_score\n",
    "\n",
    "#log.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7c153-0c5e-4da3-b213-956d40b46995",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Provided Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe69002-c07d-4cc9-9eda-a6570dc0ca86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pad_trunc_wav(nn.Module):\n",
    "    def __init__(self, max_len: int = 6*16000):\n",
    "        super(Pad_trunc_wav, self).__init__()\n",
    "        self.max_len = max_len\n",
    "    def forward(self,x):\n",
    "        shape = x.shape\n",
    "        length = shape[1]\n",
    "        if length < self.max_len:\n",
    "            multiple = self.max_len//length+1\n",
    "            x_tmp = torch.cat((x,)*multiple, axis=1)\n",
    "            x_new = x_tmp[:,0:self.max_len]\n",
    "        else:\n",
    "            x_new = x[:,0:self.max_len]\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a865a3b-4301-4e6a-be68-3e97493fd6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed=2021):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "setup_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1f002-cc58-4634-80f2-5242e4aa3174",
   "metadata": {},
   "source": [
    "### Download Required Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e52daf6-8559-477c-8e2d-55effc92fb55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChenYi\\anaconda3\\envs\\aai3001\\Lib\\site-packages\\transformers\\configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_MODEL_NAME,\n",
    "    num_labels = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f0365-cf0c-4e39-b4df-c0504fde6b78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce564f9-585e-4b73-b529-aa6ff9a334e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    def __init__(self, mode='train', max_len=6, seed=42, data_path=TSV, audio_dir=AUDIO_DIRECTORY):\n",
    "        self.mode = mode\n",
    "        data_all = pd.read_csv(data_path, sep='\\t')\n",
    "        SpkNames = np.unique(data_all['speaker'])  # ['Ses01F', 'Ses01M', ..., 'Ses05M']\n",
    "        self.data_info = self.split_dataset(data_all, SpkNames)\n",
    "        self.get_audio_dir_path = os.path.join(audio_dir)\n",
    "        self.pad_trunc = Pad_trunc_wav(max_len * 16000)\n",
    "         \n",
    "        # Label encoding\n",
    "        self.label = self.data_info['label'].astype('category').cat.codes.values\n",
    "        self.ClassNames = np.unique(self.data_info['label'])\n",
    "        self.NumClasses = len(self.ClassNames)\n",
    "        if mode == 'train':\n",
    "            print(\"Each emotion has the following number of training samples:\")\n",
    "            print([[self.ClassNames[i], (self.label == i).sum()] for i in range(self.NumClasses)])\n",
    "        self.weight = 1 / torch.tensor([(self.label == i).sum() for i in range(self.NumClasses)]).float()\n",
    "\n",
    "    def get_classname(self):\n",
    "        return self.ClassNames\n",
    "\n",
    "    def split_dataset(self, df_all, speakers):\n",
    "        test_idx = df_all['speaker'] == speakers[0]  # 'Ses01F' as test set\n",
    "        val_idx = df_all['speaker'] == speakers[1]   # 'Ses01M' as validation set\n",
    "        train_idx = ~(test_idx | val_idx)             # Remaining speakers for training\n",
    "        train_data_info = df_all[train_idx].reset_index(drop=True)\n",
    "        val_data_info = df_all[val_idx].reset_index(drop=True)\n",
    "        test_data_info = df_all[test_idx].reset_index(drop=True)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            data_info = train_data_info\n",
    "        elif self.mode == 'val':\n",
    "            data_info = val_data_info\n",
    "        elif self.mode == 'test':\n",
    "            data_info = test_data_info\n",
    "        else:\n",
    "            data_info = df_all\n",
    "        return data_info\n",
    "\n",
    "    def pre_process(self, wav):\n",
    "        wav = self.pad_trunc(wav)\n",
    "        return wav\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the raw waveform from file using data_info to get filenames\n",
    "        wav_path = os.path.join(self.get_audio_dir_path, self.data_info['filename'][idx]) + '.wav'\n",
    "        wav, sample_rate = torchaudio.load(wav_path)\n",
    "\n",
    "        # Preprocess the waveform (e.g., pad/truncate if needed)\n",
    "        wav = self.pre_process(wav)\n",
    "\n",
    "        # Apply Wav2Vec2 feature extractor\n",
    "        inputs = feature_extractor(\n",
    "            wav.squeeze().numpy(),  # Convert PyTorch tensor to numpy array\n",
    "            sampling_rate=sample_rate,\n",
    "            return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "            padding=True  # Optionally pad to a fixed length\n",
    "        )\n",
    "\n",
    "        label = self.label[idx]\n",
    "\n",
    "        # Return the processed input values and the label\n",
    "        return {\n",
    "            'input_values': inputs['input_values'].squeeze(0),  # Remove extra batch dimension\n",
    "            'labels': torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f14513-26da-4c82-91b2-8cbf4e6a040b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each emotion has the following number of training samples:\n",
      "[['A', 874], ['H', 1358], ['N', 1324], ['S', 890]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate datasets\n",
    "train_dataset = Mydataset(mode='train', max_len=6)\n",
    "val_dataset = Mydataset(mode='val', max_len=6)\n",
    "test_dataset = Mydataset(mode='test', max_len=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42062d0d-e27c-4419-93a0-7ff5e8629f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put test information into a dataframe for later use\n",
    "data_info = test_dataset.data_info\n",
    "test_dataframe = data_info[['filename', 'label']].copy()\n",
    "test_dataframe['filepath'] = test_dataframe['filename'].apply(\n",
    "    lambda x: os.path.join(test_dataset.get_audio_dir_path, f\"{x}.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a3c03-3bd8-4f17-9cb0-13e3237ead7f",
   "metadata": {},
   "source": [
    "## Model Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bea9c623-933e-454d-8a85-98cea48b0c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChenYi\\anaconda3\\envs\\aai3001\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    accuracy = np.sum(preds == labels) / len(labels)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience = EARLY_STOPPING)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Training-Output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size = BATCH,\n",
    "    per_device_eval_batch_size = BATCH,\n",
    "    num_train_epochs = EPOCH,\n",
    "    save_steps = 10,\n",
    "    save_total_limit = 5,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16 = True,\n",
    "    dataloader_pin_memory = True,\n",
    "    load_best_model_at_end = True,\n",
    "    dataloader_num_workers = NUM_WORKERS,\n",
    "    report_to = REPORT_TO,\n",
    "    gradient_accumulation_steps = GRADIENT_ACC_STEPS,\n",
    "    gradient_checkpointing = True\n",
    "    #learning_rate = LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69a846f4-aebe-4a82-ba9c-e4e8ce4e31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataframe):\n",
    "    \n",
    "    results = []\n",
    "    total = test_dataframe.shape[0]\n",
    "    count = 1\n",
    "    \n",
    "    # Run predictions on test dataset\n",
    "    if not BYPASS_MODEL_PREDICTIONN:\n",
    "        for index, row in test_dataframe.iterrows():\n",
    "\n",
    "            # Display progress\n",
    "            print(f'File {count} of {total}', end='\\r')\n",
    "            count += 1\n",
    "\n",
    "            # Load audio file\n",
    "            filename = row['filename'] + '.wav'\n",
    "            audio_file = os.path.join(AUDIO_DIRECTORY, filename)\n",
    "            y_ini, sr_ini = librosa.load(audio_file, sr = 16000)\n",
    "\n",
    "            inputs = feature_extractor(y_ini, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "            # Get the logits from the model\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "\n",
    "            # Predict the class with the highest logit value\n",
    "            predicted_class_id = torch.argmax(logits).item()\n",
    "\n",
    "            # Append the result to the list\n",
    "            results.append([row['filename'], predicted_class_id])\n",
    "\n",
    "        # Format to dataframe\n",
    "        prediction_dataframe = pd.DataFrame(results, columns=['ID', 'Predict'])\n",
    "        \n",
    "    # if user wants to directly load a csv instead of predicting\n",
    "    else:\n",
    "    \n",
    "        prediction_dataframe = pd.read_csv(PREDICTION_CSV_FILEPATH)\n",
    "\n",
    "    # Load true values\n",
    "    true_dataframe = pd.read_csv(TSV, sep='\\t')\n",
    "    remap_dict = {\n",
    "        0: 'A',\n",
    "        1: 'H',\n",
    "        2: 'N',\n",
    "        3: 'S'}\n",
    "\n",
    "    # Remap predicted values to match TSV\n",
    "    prediction_dataframe['Predict'] = prediction_dataframe['Predict'].map(remap_dict)\n",
    "\n",
    "    # Merge DataFrames on 'filename'\n",
    "    df_merged = pd.merge(true_dataframe[['filename', 'label']],prediction_dataframe[['ID', 'Predict']],\n",
    "                         left_on='filename',right_on='ID')\n",
    "\n",
    "    # Extract true labels and predictions\n",
    "    y_true = df_merged['label']\n",
    "    y_pred = df_merged['Predict']\n",
    "    \n",
    "    # Compute and print UA score\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Test UA: {macro_recall}\")\n",
    "    \n",
    "    if not JUST_PRINT_TEST_UA:\n",
    "        \n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # Create a DataFrame for the confusion matrix\n",
    "        labels = sorted(y_true.unique())\n",
    "        cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "        # Plot the confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.ylabel('Actual Labels')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        # Print the confusion matrix\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm_df)\n",
    "\n",
    "        # Compute and print classification report\n",
    "        report = classification_report(y_true, y_pred, labels=labels)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91433214-6f66-4649-b9bb-8887e4aa1b59",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start Training\n",
    "***Or load a previously trained model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f11a4d49-e762-436e-87bd-585178d12919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Previously trained model loaded from: C:\\Users\\ChenYi\\Downloads\\AAI3001_Project\\TransferLearning\\models\\wav2vec2-large-e5\n"
     ]
    }
   ],
   "source": [
    "if not LOAD_PREVIOUSLY_TRAINED_MODEL:\n",
    "    trainer.train()\n",
    "else:\n",
    "    model = Wav2Vec2ForSequenceClassification.from_pretrained(PREVIOUSLY_TRAINED_MODEL_PATH)\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(PREVIOUSLY_TRAINED_MODEL_PATH)\n",
    "    print(f\" Previously trained model loaded from: {PREVIOUSLY_TRAINED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e30a8afa-a060-474a-a9c1-bca043d86fbc",
   "metadata": {
    "id": "e30a8afa-a060-474a-a9c1-bca043d86fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test UA: 0.6808498519024835\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_PREDICT_AFTER_TRAINING:\n",
    "    test_model(model, test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbdd5eb1-2ca5-4d32-87bf-b53eb074be26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PLOT_TRAINING_GRAPHS and not LOAD_PREVIOUSLY_TRAINED_MODEL:\n",
    "    \n",
    "    train_loss_list = [train_losses.get(epoch, None) for epoch in epochs]\n",
    "    val_loss_list = [val_losses.get(epoch, None) for epoch in epochs]\n",
    "\n",
    "    # Plot only the epochs that have both losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_loss_list, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs, val_loss_list, label='Validation Loss', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AAI3001",
   "language": "python",
   "name": "aai3001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
